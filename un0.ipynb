{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHgqUtVBIOCY",
        "outputId": "3b6e2f93-4491-43f4-aa32-cbd41f62d8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat May 20 13:51:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 517.00       Driver Version: 517.00       CUDA Version: 11.7     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   58C    P8    16W /  N/A |   2222MiB /  6144MiB |     41%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1496    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
            "|    0   N/A  N/A      3764    C+G   ...Files\\Listary\\Listary.exe    N/A      |\n",
            "|    0   N/A  N/A      6868    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      7692    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
            "|    0   N/A  N/A     11660    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
            "|    0   N/A  N/A     13072    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
            "|    0   N/A  N/A     13904    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
            "|    0   N/A  N/A     17016    C+G   ...\\Fork\\app-1.84.0\\Fork.exe    N/A      |\n",
            "|    0   N/A  N/A     17084    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     17140    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     17420    C+G   ...Vantage\\LenovoVantage.exe    N/A      |\n",
            "|    0   N/A  N/A     18984      C   ...envs\\torch-env\\python.exe    N/A      |\n",
            "|    0   N/A  N/A     19184    C+G   ...774.50\\msedgewebview2.exe    N/A      |\n",
            "|    0   N/A  N/A     22324    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A     24688    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A     25640    C+G   ...774.42\\msedgewebview2.exe    N/A      |\n",
            "|    0   N/A  N/A     27584    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     28380      C   ...envs\\torch-env\\python.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi -i 0\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "print(gpu_info)\n",
        "\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from configs import model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f7escLMjIQzd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "root_path = \"D:\\Ai\\Projects\\self-supervised-learning\\data\"\n",
        "\n",
        "class CIFAR10Pair(CIFAR10):\n",
        "    \"\"\"CIFAR10 Dataset.\n",
        "    \"\"\"\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            im_1 = self.transform(img)\n",
        "            im_2 = self.transform(img)\n",
        "\n",
        "        return im_1, im_2\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "# data prepare\n",
        "train_data = CIFAR10Pair(root=root_path, train=True, transform=train_transform, download=True)\n",
        "train_dataloader = DataLoader(train_data, batch_size=model_config[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "memory_data = CIFAR10(root=root_path, train=True, transform=test_transform, download=True)\n",
        "train_val_dataloader = DataLoader(memory_data, batch_size=model_config[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "test_data = CIFAR10(root=root_path, train=False, transform=test_transform, download=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=model_config[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yaKlbMszITa5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision.utils\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "from utils import EMA\n",
        "from configs import model_config\n",
        "\n",
        "#create the Siamese Neural Network\n",
        "class MOCO(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features=512, hidden_size=4096, embedding_size=256, projection_size=256, projection_hidden_size=2048, batch_norm_mlp=True):\n",
        "        super(MOCO, self).__init__()\n",
        "        self.online = self.get_representation()\n",
        "        self.online.mean = nn.Linear(model_config[\"EMBEDDING_SIZE\"], model_config[\"EMBEDDING_SIZE\"])\n",
        "        self.online.var = nn.Linear(model_config[\"EMBEDDING_SIZE\"], model_config[\"EMBEDDING_SIZE\"])\n",
        "        self.predictor = self.get_linear_block()\n",
        "\n",
        "        self.target = self.get_target()\n",
        "        self.ema = EMA(0.999)\n",
        "\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_target(self):\n",
        "        return copy.deepcopy(self.online)\n",
        "\n",
        "    def get_linear_block(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(model_config[\"EMBEDDING_SIZE\"], model_config[\"HIDDEN_SIZE\"]),\n",
        "            nn.BatchNorm1d(model_config[\"HIDDEN_SIZE\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(model_config[\"HIDDEN_SIZE\"], model_config[\"EMBEDDING_SIZE\"])\n",
        "        )\n",
        "\n",
        "    def get_representation(self):\n",
        "        return torchvision.models.resnet50(num_classes=model_config[\"EMBEDDING_SIZE\"])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_moving_average(self):\n",
        "        for online_params, target_params in zip(self.online.parameters(), self.target.parameters()):\n",
        "            old_weight, up_weight = target_params.data, online_params.data\n",
        "            target_params.data = self.ema.update_average(old_weight, up_weight)\n",
        "            \n",
        "    def reparameterization(self, mean, logvar):\n",
        "        var = torch.exp(0.5*logvar)\n",
        "        epsilon = torch.randn_like(var)      # sampling epsilon        \n",
        "        z = mean + var * epsilon                          # reparameterization trick\n",
        "        return z\n",
        "\n",
        "    def byol_loss(self, x, y):\n",
        "        # L2 normalization\n",
        "        x = F.normalize(x, dim=-1, p=2)\n",
        "        y = F.normalize(y, dim=-1, p=2)\n",
        "        loss = 2 - 2 * (x * y).sum(dim=-1)\n",
        "        return loss\n",
        "\n",
        "    def iso_kl(self, mean, log_var):\n",
        "        # indices = find_inf_nan_indices(kl)\n",
        "        # print(indices)\n",
        "        # print(log_var)\n",
        "        # if torch.isnan(kl) or torch.isinf(kl):\n",
        "        # print(\"log_var: \", log_var)\n",
        "        # print(\"log_var.exp: \", log_var.exp())\n",
        "        # print(\"mean.pow: \", mean.pow(2))\n",
        "        return - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    def kl_divergence(self, mu1, log_var1, mu2, log_var2):\n",
        "        var1 = torch.exp(log_var1)\n",
        "        var2 = torch.exp(log_var2)\n",
        "\n",
        "        term1 = (var1 / var2 - 1).sum(dim=1)\n",
        "        term2 = ((mu2 - mu1).pow(2) / var2).sum(dim=1)\n",
        "        term3 = (log_var2 - log_var1).sum(dim=1)\n",
        "        kl_div = 0.5 * (term1 + term2 + term3)\n",
        "        \n",
        "        return kl_div.sum()\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        embedding_o = self.online(x)\n",
        "        mean_o = self.online.mean(self.LeakyReLU(embedding_o))\n",
        "        logvar_o = self.online.var(self.LeakyReLU(embedding_o)),\n",
        "        z_o = self.reparameterization(mean_o, logvar_o)\n",
        "        z_o_p = self.predictor(z_o)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding_tar = self.target(x).detach()\n",
        "            mean_tar = self.target.mean(self.LeakyReLU(embedding_tar)).detach()\n",
        "            logvar_tar = self.target.var(self.LeakyReLU(embedding_tar)).detach()\n",
        "            z_tar = self.reparameterization(mean_tar, logvar_tar).detach()\n",
        "\n",
        "        distance_loss = self.byol_loss(z_o_p, z_tar)\n",
        "\n",
        "        kl_loss = self.kl_divergence(mean_o, logvar_o, mean_tar, logvar_tar)\n",
        "\n",
        "        iso_kl_loss = self.iso_kl(mean_o, logvar_o)\n",
        "        iso_kl_loss += self.iso_kl(mean_tar, logvar_tar)\n",
        "\n",
        "        # if torch.isnan(kl_loss) or torch.isinf(kl_loss):\n",
        "        #     print(\"------------------------\")\n",
        "        #     print(\"kl_total: \", kl_loss)\n",
        "            # print(\"logvar_o: \", logvar_o)\n",
        "            # print(\"logvar_tar: \", logvar_tar)\n",
        "\n",
        "\n",
        "        return kl_loss, distance_loss, iso_kl_loss, embedding_o\n",
        "        \n",
        "\n",
        "    def forward(self, x1, x2=None):\n",
        "        if x2 is None:\n",
        "            return self.online(x1)\n",
        "\n",
        "        kl_loss1, distance_loss1, iso_kl_loss1, embedding_o1 = self.forward_once(x1)\n",
        "        kl_loss2, distance_loss2, iso_kl_loss2, embedding_o2 = self.forward_once(x2)\n",
        "\n",
        "        \n",
        "\n",
        "        kl_total = kl_loss1 + kl_loss2\n",
        "        iso_kl_total = iso_kl_loss1 + iso_kl_loss2\n",
        "        distance_total = (distance_loss1 + distance_loss2).mean()\n",
        "\n",
        "        total_loss =  distance_total  + iso_kl_total\n",
        "\n",
        "        print(\"kl_loss: \", kl_total)\n",
        "        print(\"distance_total: \", distance_total)\n",
        "        print(\"iso_kl_total: \", iso_kl_total)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "def find_inf_nan_indices(tensor):\n",
        "    # Check for inf and nan values\n",
        "    is_inf = torch.isinf(tensor)\n",
        "    is_nan = torch.isnan(tensor)\n",
        "\n",
        "    # Combine the masks to find the indices where either inf or nan is present\n",
        "    inf_nan_mask = torch.logical_or(is_inf, is_nan)\n",
        "\n",
        "    # Get the indices where inf or nan is present\n",
        "    indices = torch.nonzero(inf_nan_mask)\n",
        "\n",
        "    return indices\n",
        "# temp1 = torch.rand((6, 3, 32, 32))\n",
        "# temp2 = torch.rand((6, 3, 32, 32))\n",
        "# temp_model = MOCO()\n",
        "# ress = temp_model(temp1, temp2)[0]\n",
        "# for res in ress:\n",
        "#     print(res[0].size(), res[1].size(), res[2].size(), res[3].size())\n",
        "model = MOCO().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eOLe996HIVkl"
      },
      "outputs": [],
      "source": [
        "# train for one epoch\n",
        "def train(net, data_loader, train_optimizer, epoch):\n",
        "    net.train()\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
        "    for im_1, im_2 in train_bar:\n",
        "        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n",
        "\n",
        "        loss = net(im_1, im_2)\n",
        "        \n",
        "        train_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_optimizer.step()\n",
        "\n",
        "        net.update_moving_average()\n",
        "\n",
        "        total_num += data_loader.batch_size\n",
        "        total_loss += loss.item() * data_loader.batch_size\n",
        "        train_bar.set_description('Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(epoch, model_config[\"EPOCHS\"], optimizer.param_groups[0]['lr'], total_loss / total_num))\n",
        "\n",
        "    return total_loss / total_num\n",
        "\n",
        "# lr scheduler for training\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    lr = model_config[\"LEARNING_RATE\"]\n",
        "    if True:  # cosine lr schedule\n",
        "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / model_config[\"EPOCHS\"]))\n",
        "    else:  # stepwise lr schedule\n",
        "        for milestone in args.schedule:\n",
        "            lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c-kEZF96IWrV"
      },
      "outputs": [],
      "source": [
        "# test using a knn monitor\n",
        "def test(net, memory_data_loader, test_data_loader, epoch):\n",
        "    net.eval()\n",
        "    classes = len(memory_data_loader.dataset.classes)\n",
        "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "    with torch.no_grad():\n",
        "        # generate feature bank\n",
        "        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
        "            feature = net(data.cuda(non_blocking=True))\n",
        "            feature = F.normalize(feature, dim=1)\n",
        "            feature_bank.append(feature)\n",
        "        # [D, N]\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "        # [N]\n",
        "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_data_loader)\n",
        "        for data, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            feature = net(data)\n",
        "            feature = F.normalize(feature, dim=1)\n",
        "            \n",
        "            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, 200, 0.1)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
        "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, model_config[\"EPOCHS\"], total_top1 / total_num * 100))\n",
        "\n",
        "    return total_top1 / total_num * 100\n",
        "\n",
        "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
        "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
        "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n",
        "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "    sim_matrix = torch.mm(feature, feature_bank)\n",
        "    # [B, K]\n",
        "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
        "    # [B, K]\n",
        "    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n",
        "    sim_weight = (sim_weight / knn_t).exp()\n",
        "\n",
        "    # counts for each class\n",
        "    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n",
        "    # [B*K, C]\n",
        "    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
        "    # weighted score ---> [B, C]\n",
        "    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "    return pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V8KVSLqdIXrp"
      },
      "outputs": [],
      "source": [
        "def get_params_groups(model):\n",
        "    regularized = []\n",
        "    not_regularized = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            # print(name)\n",
        "            continue\n",
        "        # we do not regularize biases nor Norm parameters\n",
        "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
        "            not_regularized.append(param)\n",
        "        else:\n",
        "            regularized.append(param)\n",
        "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iMObTkWIZIu",
        "outputId": "87937cd5-30a2-45a8-9e6e-5c771df1a7df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kl_loss:  tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "distance_total:  tensor(4.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iso_kl_total:  tensor(66691.3438, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/800], lr: 0.050000, Loss: 66695.3672:   1%|          | 1/97 [00:03<06:17,  3.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kl_loss:  tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "distance_total:  tensor(2.4646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iso_kl_total:  tensor(1.8806e+20, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/800], lr: 0.050000, Loss: 94027938627124756480.0000:   2%|▏         | 2/97 [00:05<03:55,  2.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kl_loss:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "distance_total:  tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iso_kl_total:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/800], lr: 0.050000, Loss: nan:   3%|▎         | 3/97 [00:06<03:06,  1.99s/it]                      "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kl_loss:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "distance_total:  tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iso_kl_total:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/800], lr: 0.050000, Loss: nan:   4%|▍         | 4/97 [00:08<02:42,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kl_loss:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "distance_total:  tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iso_kl_total:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/800], lr: 0.050000, Loss: nan:   5%|▌         | 5/97 [00:09<02:59,  1.96s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\Ai\\Projects\\self-supervised-learning\\sss\\un0.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# if not os.path.exists(model_config[\"SAVE_DIR\"]):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     os.mkdir(model_config[\"SAVE_DIR\"])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# dump args\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# training loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_start, model_config[\u001b[39m\"\u001b[39m\u001b[39mEPOCHS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, optimizer, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     test_acc_1 \u001b[39m=\u001b[39m test(model\u001b[39m.\u001b[39monline, train_val_dataloader, test_dataloader, epoch)\n",
            "\u001b[1;32md:\\Ai\\Projects\\self-supervised-learning\\sss\\un0.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, data_loader, train_optimizer, epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m adjust_learning_rate(optimizer, epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m total_loss, total_num, train_bar \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0\u001b[39m, tqdm(data_loader)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m im_1, im_2 \u001b[39min\u001b[39;00m train_bar:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     im_1, im_2 \u001b[39m=\u001b[39m im_1\u001b[39m.\u001b[39mcuda(non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), im_2\u001b[39m.\u001b[39mcuda(non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m net(im_1, im_2)\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;32md:\\Ai\\Projects\\self-supervised-learning\\sss\\un0.ipynb Cell 7\u001b[0m in \u001b[0;36mCIFAR10Pair.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     im_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     im_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ai/Projects/self-supervised-learning/sss/un0.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m im_1, im_2\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:535\u001b[0m, in \u001b[0;36mRandomApply.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[0;32m    534\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m--> 535\u001b[0m     img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m    536\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\IT CITY\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1238\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1233\u001b[0m fn_idx, brightness_factor, contrast_factor, saturation_factor, hue_factor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\n\u001b[0;32m   1234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbrightness, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrast, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaturation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhue\n\u001b[0;32m   1235\u001b[0m )\n\u001b[0;32m   1237\u001b[0m \u001b[39mfor\u001b[39;00m fn_id \u001b[39min\u001b[39;00m fn_idx:\n\u001b[1;32m-> 1238\u001b[0m     \u001b[39mif\u001b[39;00m fn_id \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39mand\u001b[39;00m brightness_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1239\u001b[0m         img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madjust_brightness(img, brightness_factor)\n\u001b[0;32m   1240\u001b[0m     \u001b[39melif\u001b[39;00m fn_id \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m contrast_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.target.requires_grad_(False)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.SGD(get_params_groups(model), lr=0.06, weight_decay=5e-4, momentum=0.9)\n",
        "\n",
        "# load model if resume\n",
        "epoch_start = 1\n",
        "\n",
        "# if resume is not '':\n",
        "#     checkpoint = torch.load(resume)\n",
        "#     model.load_state_dict(checkpoint['state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "#     epoch_start = checkpoint['epoch'] + 1\n",
        "#     print('Loaded from: {}'.format(model_config[\"RESUME\"]))\n",
        "\n",
        "# logging\n",
        "results = {'train_loss': [], 'test_acc@1': []}\n",
        "# if not os.path.exists(model_config[\"SAVE_DIR\"]):\n",
        "#     os.mkdir(model_config[\"SAVE_DIR\"])\n",
        "# dump args\n",
        "# with open(results_dir + '/args.json', 'w') as fid:\n",
        "#     json.dump(__dict__, fid, indent=2)\n",
        "\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epoch_start, model_config[\"EPOCHS\"] + 1):\n",
        "    train_loss = train(model, train_dataloader, optimizer, epoch)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    test_acc_1 = test(model.online, train_val_dataloader, test_dataloader, epoch)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n",
        "    # data_frame.to_csv(model_config[\"SAVE_DIR\"] + '/log.csv', index_label='epoch')\n",
        "    # save model\n",
        "    # torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, model_config[\"SAVE_DIR\"] + '/model_last.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNsl8sl8Ia5K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
